{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def process_problem_data(base_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Iterates through all problem directories, extracts problem statements\n",
    "\n",
    "    and sentences from `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        base_path (str): The path to the directory containing all the problems\n",
    "\n",
    "                         (e.g., 'math-rollouts/.../correct_base_solution').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: A list of dictionaries, where each dictionary contains the problem\n",
    "\n",
    "              and all sentences for a given problem directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_problem_data = []\n",
    "\n",
    "\n",
    "    # Check if the base path exists\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "    print(f\"Found problem directory: {base_path}\")\n",
    "\n",
    "\n",
    "    # List all entries in the base directory\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "    print(f\"Found problem directory: {problem_dirs}\")\n",
    "\n",
    "\n",
    "    if not problem_dirs:\n",
    "\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "\n",
    "    # Iterate through each problem directory (e.g., problem_330, problem_1591)\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "\n",
    "       \n",
    "\n",
    "        # Define the file paths for the problem and chunks\n",
    "\n",
    "        problem_file = os.path.join(problem_path, \"problem.json\")\n",
    "\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "\n",
    "       \n",
    "\n",
    "        problem_text = \"\"\n",
    "\n",
    "        allsentences = []\n",
    "\n",
    "       \n",
    "\n",
    "        # Load the problem statement\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(problem_file, 'r') as f:\n",
    "\n",
    "                problem_data = json.load(f)\n",
    "\n",
    "                problem_text = problem_data.get(\"problem\", \"\")\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load problem.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Load all sentences from chunks_labeled.json\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(chunks_file, 'r') as f:\n",
    "\n",
    "                chunks_data = json.load(f)\n",
    "\n",
    "                allsentences = [chunk[\"chunk\"] for chunk in chunks_data]\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load chunks_labeled.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted data\n",
    "\n",
    "        problem_info = {\n",
    "\n",
    "            \"problem_id\": problem_name,\n",
    "\n",
    "            \"problem_statement\": problem_text,\n",
    "\n",
    "            \"sentences\": allsentences\n",
    "\n",
    "        }\n",
    "\n",
    "        all_problem_data.append(problem_info)\n",
    "\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "correct_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "# Now, `all_data` is a list of dictionaries. You can iterate through it.\n",
    "\n",
    "print(f\"Successfully loaded data for {len(correct_all_data)} problems.\")\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "incorrect_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(incorrect_all_data)} problems.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = correct_all_data + incorrect_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True,  torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6352bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: Sentence boundaries using tokenizer ---\n",
    "def get_raw_tokens(text, model_name=None):\n",
    "    return tokenizer(text)['input_ids']\n",
    "\n",
    "def get_sentence_token_boundaries(text, sentences, model_name=None):\n",
    "    import re\n",
    "    def normalize_spaces(s):\n",
    "        return re.sub(r\"[\\u00A0\\u1680\\u2000-\\u200B\\u202F\\u205F\\u3000\\uFEFF]\", \" \", s)\n",
    "    char_positions = []\n",
    "    search_start = 0\n",
    "    text_normalized = normalize_spaces(text)\n",
    "    for sentence in sentences:\n",
    "        sentence_normalized = normalize_spaces(sentence)\n",
    "        norm_pos = text_normalized.find(sentence_normalized, search_start)\n",
    "        if norm_pos == -1:\n",
    "            sentence_stripped = sentence_normalized.strip()\n",
    "            norm_pos = text_normalized.find(sentence_stripped, search_start)\n",
    "            if norm_pos == -1:\n",
    "                raise ValueError(f\"Sentence not found in text: {sentence}\")\n",
    "            norm_end = norm_pos + len(sentence_stripped)\n",
    "        else:\n",
    "            norm_end = norm_pos + len(sentence_normalized)\n",
    "        original_pos = 0\n",
    "        normalized_count = 0\n",
    "        actual_start = -1\n",
    "        actual_end = -1\n",
    "        for i, char in enumerate(text):\n",
    "            if normalized_count == norm_pos and actual_start == -1:\n",
    "                actual_start = i\n",
    "            if normalized_count == norm_end:\n",
    "                actual_end = i\n",
    "                break\n",
    "            if normalize_spaces(char) == \" \" or char == text_normalized[normalized_count]:\n",
    "                normalized_count += 1\n",
    "        if actual_end == -1 and normalized_count == norm_end:\n",
    "            actual_end = len(text)\n",
    "        char_positions.append((actual_start, actual_end))\n",
    "        search_start = norm_end\n",
    "    token_boundaries = []\n",
    "    for char_start, char_end in char_positions:\n",
    "        tokens_to_start = len(get_raw_tokens(text[:char_start], model_name)) if char_start > 0 else 0\n",
    "        tokens_to_end = len(get_raw_tokens(text[:char_end], model_name))\n",
    "        token_boundaries.append((tokens_to_start, tokens_to_end))\n",
    "    return token_boundaries\n",
    "\n",
    "# --- Helper: Average attention over sentence boundaries ---\n",
    "def _compute_averaged_matrix(matrix, sentence_boundaries):\n",
    "    n = len(sentence_boundaries)\n",
    "    result = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        row_start, row_end = sentence_boundaries[i]\n",
    "        row_start = min(row_start, matrix.shape[0] - 1)\n",
    "        row_end = min(row_end, matrix.shape[0] - 1)\n",
    "        if row_start >= row_end:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            col_start, col_end = sentence_boundaries[j]\n",
    "            col_start = min(col_start, matrix.shape[1] - 1)\n",
    "            col_end = min(col_end, matrix.shape[1] - 1)\n",
    "            if col_start >= col_end:\n",
    "                continue\n",
    "            region = matrix[row_start:row_end, col_start:col_end]\n",
    "            if region.size > 0:\n",
    "                result[i, j] = np.mean(region)\n",
    "    return result\n",
    "\n",
    "# --- Helper: Get vertical scores (receiver heads) ---\n",
    "def get_vertical_scores(avg_mat, proximity_ignore=1, control_depth=False, score_type=\"mean\"):\n",
    "    n = avg_mat.shape[0]\n",
    "    trius = np.triu_indices_from(avg_mat, k=1)\n",
    "    avg_mat = avg_mat.copy()\n",
    "    avg_mat[trius] = np.nan\n",
    "    trils = np.triu_indices_from(avg_mat, k=-proximity_ignore + 1)\n",
    "    avg_mat[trils] = np.nan\n",
    "    if control_depth:\n",
    "        per_row = np.sum(~np.isnan(avg_mat), axis=1)\n",
    "        avg_mat = stats.rankdata(avg_mat, axis=1, nan_policy=\"omit\") / per_row[:, None]\n",
    "    n = avg_mat.shape[-1]\n",
    "    vert_scores = []\n",
    "    for i in range(n):\n",
    "        vert_lines = avg_mat[i + proximity_ignore :, i]\n",
    "        if score_type == \"mean\":\n",
    "            vert_score = np.nanmean(vert_lines)\n",
    "        elif score_type == \"median\":\n",
    "            vert_score = np.nanmedian(vert_lines)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown score_type: {score_type}\")\n",
    "        vert_scores.append(vert_score)\n",
    "    return np.array(vert_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_ar_kurtosis(all_layer_head_vert_scores):\n",
    "    \"\"\"\n",
    "    Compute kurtosis across the last axis (sentences) for each (layer, head).\n",
    "    Input: all_layer_head_vert_scores: shape (num_layers, num_heads, num_sentences)\n",
    "    Output: layer_head_kurtosis: shape (num_layers, num_heads)\n",
    "    \"\"\"\n",
    "    return stats.kurtosis(\n",
    "        all_layer_head_vert_scores, axis=2, fisher=True, bias=True, nan_policy=\"omit\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_problems_for_averaged_kurtosis_simple(all_data, model, tokenizer, model_name, batch_size=2):\n",
    "    \"\"\"\n",
    "    Process all problems and compute averaged kurtosis across reasoning traces.\n",
    "    Uses your existing loaded model. No saving of individual vertical scores.\n",
    "    \"\"\"\n",
    "    all_problem_vert_scores = []\n",
    "    successful_problems = 0\n",
    "    \n",
    "    print(f\"Processing {len(all_data)} problems for averaged kurtosis analysis...\")\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(all_data), batch_size), desc=\"Processing batches\"):\n",
    "        batch_end = min(batch_start + batch_size, len(all_data))\n",
    "        batch_problems = all_data[batch_start:batch_end]\n",
    "        \n",
    "        for i, problem in enumerate(batch_problems):\n",
    "            global_idx = batch_start + i\n",
    "            print(f\"Processing problem {global_idx+1}/{len(all_data)}: {problem['problem_id']}\")\n",
    "            \n",
    "            try:\n",
    "                # Extract problem data\n",
    "                problem_statement = problem['problem_statement']\n",
    "                sentences = problem['sentences']\n",
    "                \n",
    "                # Limit sentence count to avoid memory issues\n",
    "                max_sentences = 100\n",
    "                if len(sentences) > max_sentences:\n",
    "                    sentences = sentences[:max_sentences]\n",
    "                \n",
    "                # Create input text with problem statement first\n",
    "                all_sentences = [problem_statement] + sentences\n",
    "                input_text = \"\\n\".join(all_sentences)\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "                \n",
    "                # Move to GPU if available\n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                # Get attention weights\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs, output_attentions=True)\n",
    "                attention_weights = outputs.attentions\n",
    "                \n",
    "                # Get sentence boundaries - use the function you already have\n",
    "                sentence_boundaries = get_sentence_token_boundaries(\n",
    "                    input_text, all_sentences, model_name\n",
    "                )\n",
    "                \n",
    "                if len(sentence_boundaries) == 0:\n",
    "                    print(f\"  FAILED: No sentence boundaries found\")\n",
    "                    continue\n",
    "                \n",
    "                # Get dimensions\n",
    "                num_layers = len(attention_weights)\n",
    "                num_heads = attention_weights[0].shape[1]\n",
    "                num_sentences = len(sentence_boundaries)\n",
    "                \n",
    "                # Initialize array for vertical scores\n",
    "                all_layer_head_vert_scores = np.zeros((num_layers, num_heads, num_sentences), dtype=np.float32)\n",
    "                \n",
    "                for layer in range(num_layers):\n",
    "                    for head in range(num_heads):\n",
    "                        # Extract attention tensor and convert to float32 if needed\n",
    "                        attn_tensor = attention_weights[layer][0, head]\n",
    "                        if attn_tensor.dtype == torch.bfloat16:\n",
    "                            attn_tensor = attn_tensor.to(torch.float32)\n",
    "                        \n",
    "                        attn_mat = attn_tensor.detach().cpu().numpy()\n",
    "                        \n",
    "                        # Compute vertical scores\n",
    "                        avg_mat = _compute_averaged_matrix(attn_mat, sentence_boundaries)\n",
    "                        vert_scores = get_vertical_scores(\n",
    "                            avg_mat, proximity_ignore=1, control_depth=False, score_type=\"mean\"\n",
    "                        )\n",
    "                        \n",
    "                        # Store vertical scores\n",
    "                        actual_len = min(len(vert_scores), num_sentences)\n",
    "                        all_layer_head_vert_scores[layer, head, :actual_len] = vert_scores[:actual_len]\n",
    "                \n",
    "                # Store this problem's vertical scores\n",
    "                all_problem_vert_scores.append(all_layer_head_vert_scores)\n",
    "                successful_problems += 1\n",
    "                print(f\"  SUCCESS: Shape {all_layer_head_vert_scores.shape}. Total successful: {successful_problems}\")\n",
    "                \n",
    "                # Clean up memory\n",
    "                del outputs, attention_weights\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  FAILED: Error processing problem {problem['problem_id']}: {e}\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                continue\n",
    "        \n",
    "        # Force garbage collection after each batch\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Completed batch {batch_start//batch_size + 1}/{(len(all_data)-1)//batch_size + 1}\")\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {successful_problems} problems\")\n",
    "    \n",
    "    if not all_problem_vert_scores:\n",
    "        print(\"No vertical scores collected!\")\n",
    "        return None\n",
    "    \n",
    "    # Compute averaged kurtosis (this should give lower values around 40)\n",
    "    print(\"Computing averaged kurtosis across all reasoning traces...\")\n",
    "    \n",
    "    # Find minimum sentence count across all problems\n",
    "    min_sentences = min(scores.shape[2] for scores in all_problem_vert_scores)\n",
    "    print(f\"Minimum sentences across all problems: {min_sentences}\")\n",
    "    \n",
    "    # Truncate all problems to same sentence length\n",
    "    truncated_scores = []\n",
    "    for scores in all_problem_vert_scores:\n",
    "        truncated_scores.append(scores[:, :, :min_sentences])\n",
    "    \n",
    "    # Stack and average across problems\n",
    "    stacked_scores = np.stack(truncated_scores, axis=0)  # Shape: (num_problems, num_layers, num_heads, min_sentences)\n",
    "    averaged_scores = np.mean(stacked_scores, axis=0)    # Shape: (num_layers, num_heads, min_sentences)\n",
    "    \n",
    "    print(f\"Averaged scores shape: {averaged_scores.shape}\")\n",
    "    \n",
    "    # Use the kurtosis function\n",
    "    layer_head_kurtosis = get_3d_ar_kurtosis(averaged_scores)\n",
    "    \n",
    "    print(f\"Kurtosis computation complete!\")\n",
    "    print(f\"Kurtosis shape: {layer_head_kurtosis.shape}\")\n",
    "    print(f\"Kurtosis range: {np.nanmin(layer_head_kurtosis):.3f} to {np.nanmax(layer_head_kurtosis):.3f}\")\n",
    "    print(f\"Kurtosis mean: {np.nanmean(layer_head_kurtosis):.3f}\")\n",
    "    \n",
    "    return layer_head_kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84147ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "layer_head_kurtosis = process_all_problems_for_averaged_kurtosis_simple(\n",
    "    all_data, model, tokenizer, model_name, batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "flat_kurtosis = layer_head_kurtosis.flatten()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(flat_kurtosis, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Kurtosis Scores for All (Layer, Head) Pairs')\n",
    "plt.xlabel('Kurtosis')\n",
    "plt.ylabel('Count')\n",
    "# Plot percentiles\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    perc = np.percentile(flat_kurtosis, p)\n",
    "    plt.axvline(perc, color='red', linestyle='--', label=f'{p}th percentile: {perc:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top = 20  # Number of heads to select\n",
    "# Flatten and get indices of top 20 kurtosis values\n",
    "flat_indices = np.argsort(layer_head_kurtosis.flatten())[::-1][:num_top]\n",
    "layer_indices, head_indices = np.unravel_index(flat_indices, layer_head_kurtosis.shape)\n",
    "top_20_heads = list(zip(layer_indices, head_indices))\n",
    "\n",
    "print(\"Top 20 heads by kurtosis (layer, head):\")\n",
    "for i, (layer, head) in enumerate(top_20_heads):\n",
    "    print(f\"{i+1:2d}: Layer {layer}, Head {head}, Kurtosis: {layer_head_kurtosis[layer, head]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_heads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
