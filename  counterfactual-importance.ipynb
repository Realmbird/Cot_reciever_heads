{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def process_problem_data(base_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Iterates through all problem directories, extracts problem statements\n",
    "\n",
    "    and sentences from `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        base_path (str): The path to the directory containing all the problems\n",
    "\n",
    "                         (e.g., 'math-rollouts/.../correct_base_solution').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: A list of dictionaries, where each dictionary contains the problem\n",
    "\n",
    "              and all sentences for a given problem directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_problem_data = []\n",
    "\n",
    "\n",
    "    # Check if the base path exists\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "    print(f\"Found problem directory: {base_path}\")\n",
    "\n",
    "\n",
    "    # List all entries in the base directory\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "    print(f\"Found problem directory: {problem_dirs}\")\n",
    "\n",
    "\n",
    "    if not problem_dirs:\n",
    "\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "\n",
    "    # Iterate through each problem directory (e.g., problem_330, problem_1591)\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "\n",
    "       \n",
    "\n",
    "        # Define the file paths for the problem and chunks\n",
    "\n",
    "        problem_file = os.path.join(problem_path, \"problem.json\")\n",
    "\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "\n",
    "       \n",
    "\n",
    "        problem_text = \"\"\n",
    "\n",
    "        allsentences = []\n",
    "\n",
    "       \n",
    "\n",
    "        # Load the problem statement\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(problem_file, 'r') as f:\n",
    "\n",
    "                problem_data = json.load(f)\n",
    "\n",
    "                problem_text = problem_data.get(\"problem\", \"\")\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load problem.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Load all sentences from chunks_labeled.json\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(chunks_file, 'r') as f:\n",
    "\n",
    "                chunks_data = json.load(f)\n",
    "\n",
    "                allsentences = [chunk[\"chunk\"] for chunk in chunks_data]\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load chunks_labeled.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted data\n",
    "\n",
    "        problem_info = {\n",
    "\n",
    "            \"problem_id\": problem_name,\n",
    "\n",
    "            \"problem_statement\": problem_text,\n",
    "\n",
    "            \"sentences\": allsentences\n",
    "\n",
    "        }\n",
    "\n",
    "        all_problem_data.append(problem_info)\n",
    "\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "correct_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "# Now, `all_data` is a list of dictionaries. You can iterate through it.\n",
    "\n",
    "print(f\"Successfully loaded data for {len(correct_all_data)} problems.\")\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "incorrect_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(incorrect_all_data)} problems.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f077a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompt = correct_all_data + incorrect_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_problem_labels_minimal(base_path):\n",
    "    \"\"\"\n",
    "    Iterates through all problem directories, extracts selected fields from each chunk\n",
    "    in `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The path to the directory containing all the problems.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each with problem_id and a list of selected chunk fields.\n",
    "    \"\"\"\n",
    "    all_problem_data = []\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "    if not problem_dirs:\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "        return all_problem_data\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "        if not os.path.isfile(chunks_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(chunks_file, \"r\") as f:\n",
    "                chunks_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {problem_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract only the fields you care about from each chunk\n",
    "        selected_chunks = []\n",
    "        for chunk in chunks_data:\n",
    "            selected = {\n",
    "                \"function_tags\": chunk.get(\"function_tags\"),\n",
    "                \"chunk\": chunk.get(\"chunk\"),\n",
    "                \"accuracy\": chunk.get(\"accuracy\"),\n",
    "                \"resampling_importance_accuracy\": chunk.get(\"resampling_importance_accuracy\"),\n",
    "                \"resampling_importance_kl\": chunk.get(\"resampling_importance_kl\"),\n",
    "                \"counterfactual_importance_accuracy\": chunk.get(\"counterfactual_importance_accuracy\"),\n",
    "                \"counterfactual_importance_kl\": chunk.get(\"counterfactual_importance_kl\"),\n",
    "                \n",
    "                \"summary\": chunk.get(\"summary\"),\n",
    "            }\n",
    "            selected_chunks.append(selected)\n",
    "\n",
    "        all_problem_data.append({\n",
    "            \"problem_id\": problem_name,\n",
    "            \"chunks\": selected_chunks\n",
    "        })\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "# Example usage:\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "correct_all_data = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(correct_all_data)} problems with selected chunk fields.\")\n",
    "base_problem_dir = \"math-rollouts/deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "incorrect_all_data = process_problem_labels_minimal(base_problem_dir)\n",
    "print(f\"Loaded {len(incorrect_all_data)} problems with selected chunk fields.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_labels = correct_all_data + incorrect_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c146b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80bbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ed2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#want do a loop through all the chunks append it to the current text set it through multiple rollouts and measure counterfactual importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6ded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e957192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    a: shape (n_samples, n_features)\n",
    "    b: shape (1, n_features) or (n_features,)\n",
    "    Returns: shape (n_samples,)\n",
    "    \"\"\"\n",
    "    a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = np.linalg.norm(b)\n",
    "    sim = np.dot(a, b.reshape(-1)) / (a_norm.flatten() * b_norm + 1e-8)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff029b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ca18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_problem_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196675f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"Normalize numerical answer for comparison.\"\"\"\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    answer = str(answer).strip()\n",
    "    \n",
    "    # Remove common prefixes/suffixes\n",
    "    answer = re.sub(r'^(the answer is|answer:|final answer:)\\s*', '', answer.lower())\n",
    "    answer = re.sub(r'\\s*(dollars?|cents?|\\$|€|£)\\s*$', '', answer)\n",
    "    \n",
    "    # Extract numerical value\n",
    "    number_match = re.search(r'([+-]?\\d*\\.?\\d+)', answer)\n",
    "    if number_match:\n",
    "        try:\n",
    "            # Try to convert to float then back to remove trailing zeros\n",
    "            num = float(number_match.group(1))\n",
    "            if num.is_integer():\n",
    "                return str(int(num))\n",
    "            else:\n",
    "                return str(num)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If no number found, return original cleaned answer\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "def extract_final_answer(text):\n",
    "    \"\"\"Extract the final numerical answer from model output.\"\"\"\n",
    "    patterns = [\n",
    "        r\"(?:the answer is|answer:|final answer:)\\s*([+-]?\\d*\\.?\\d+)\",\n",
    "        r\"([+-]?\\d*\\.?\\d+)\\s*$\",  # Number at end\n",
    "        r\"\\\\boxed\\{([^}]+)\\}\",    # LaTeX boxed answer\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text.lower())\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def get_ground_truth_answer(problem_id, base_dirs):\n",
    "    \"\"\"Get ground truth answer from problem.json\"\"\"\n",
    "    for base_dir in base_dirs:\n",
    "        problem_path = f\"{base_dir}/{problem_id}/problem.json\"\n",
    "        try:\n",
    "            with open(problem_path, 'r') as f:\n",
    "                problem_data = json.load(f)\n",
    "                return problem_data.get(\"answer\", None)\n",
    "        except:\n",
    "            continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(model, inputs):\n",
    "    \"\"\"Get probability distribution from model.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits[0, -1, :]  # Last token logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, inputs, layer=-1):\n",
    "    \"\"\"Get hidden state embeddings from a specific layer.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # Get embeddings and convert to float32 on GPU\n",
    "        embeddings = outputs.hidden_states[layer][0, -1, :].float()  # Convert to float32\n",
    "    \n",
    "    return embeddings  # Keep on GPU\n",
    "\n",
    "def generate_diverse_rollouts(model, tokenizer, context, num_rollouts=10, batch_size=5, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"Generate diverse text completions in batches for better GPU utilization.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\", max_length=1500, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    rollout_texts = []\n",
    "    rollout_embeddings = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_start in range(0, num_rollouts, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, num_rollouts)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "        \n",
    "        # Expand inputs for batch processing\n",
    "        batch_inputs = {\n",
    "            'input_ids': inputs['input_ids'].repeat(current_batch_size, 1),\n",
    "            'attention_mask': inputs['attention_mask'].repeat(current_batch_size, 1)\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                batch_inputs['input_ids'],\n",
    "                attention_mask=batch_inputs['attention_mask'],\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        # Process each sequence in the batch\n",
    "        for i in range(current_batch_size):\n",
    "            # Decode generated text\n",
    "            generated_ids = outputs.sequences[i][inputs['input_ids'].shape[1]:]\n",
    "            generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            rollout_texts.append(generated_text.strip())\n",
    "            \n",
    "            # Get embeddings and convert to float32, keep on GPU\n",
    "            final_hidden = outputs.hidden_states[-1][-1][i, -1, :].float().cpu()  # Convert to float32\n",
    "            rollout_embeddings.append(final_hidden)\n",
    "    \n",
    "    # Stack embeddings into a single tensor on GPU\n",
    "    return rollout_texts, torch.stack(rollout_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d35311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated main processing loop\n",
    "num_rollouts = 10  # Reduced for actual generation\n",
    "cos_sim_threshold = 0.8\n",
    "results = []\n",
    "for prompt, label in zip(all_prompt[:1], all_problem_labels[:1]):  # Test with 1 problem first\n",
    "    problem_id = prompt[\"problem_id\"]\n",
    "    problem_text = prompt[\"problem_statement\"]\n",
    "    allsentences = prompt[\"sentences\"]\n",
    "   \n",
    "    # Get baseline (original full context) - CONVERT TO NUMPY\n",
    "    context_original = problem_text + \" \" + \" \".join(allsentences)\n",
    "    inputs_original = tokenizer(context_original, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    original_embedding = get_embeddings(model, inputs_original).cpu().numpy()  # ADD .cpu().numpy()\n",
    "   \n",
    "    for i, chunk in enumerate(allsentences[:5]):  # Test first 5 chunks\n",
    "        print(f\"Processing chunk {i+1}/{min(5, len(allsentences))}: {chunk[:50]}...\")\n",
    "       \n",
    "        # Context WITHOUT the chunk (this is what thought-anchors does)\n",
    "        chunks_without = allsentences[:i] + allsentences[i+1:]\n",
    "        context_without = problem_text + \" \" + \" \".join(chunks_without)\n",
    "       \n",
    "        # Generate diverse rollouts from context without chunk\n",
    "        rollout_texts, rollout_embeddings = generate_diverse_rollouts(\n",
    "            model, tokenizer, context_without, num_rollouts=num_rollouts\n",
    "        )\n",
    "       \n",
    "        # CONVERT ROLLOUT EMBEDDINGS TO NUMPY IF THEY'RE TENSORS\n",
    "        if torch.is_tensor(rollout_embeddings):\n",
    "            rollout_embeddings = rollout_embeddings.cpu().numpy()\n",
    "       \n",
    "        # Calculate cosine similarities between rollouts and original\n",
    "        cos_sims = []\n",
    "        for rollout_emb in rollout_embeddings:\n",
    "            cos_sim = np.dot(original_embedding, rollout_emb) / (\n",
    "                np.linalg.norm(original_embedding) * np.linalg.norm(rollout_emb) + 1e-8\n",
    "            )\n",
    "            cos_sims.append(cos_sim)\n",
    "       \n",
    "        cos_sims = np.array(cos_sims)\n",
    "        similar_mask = cos_sims > cos_sim_threshold\n",
    "        not_similar_mask = ~similar_mask\n",
    "       \n",
    "        # Calculate importance metrics\n",
    "        avg_cos_sim = np.mean(cos_sims)\n",
    "        num_different = int(not_similar_mask.sum())\n",
    "       \n",
    "        # KL divergence between \"different\" rollouts and original (if any different ones exist)\n",
    "        kl_divergence = None\n",
    "        if num_different > 0:\n",
    "            # For embeddings, we can use cosine distance as a proxy for KL\n",
    "            different_cos_sims = cos_sims[not_similar_mask]\n",
    "            kl_divergence = float(np.mean(1 - different_cos_sims))  # 1 - cosine similarity\n",
    "       \n",
    "        # Count unique responses\n",
    "        unique_responses = len(set(rollout_texts))\n",
    "       \n",
    "        results.append({\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_text\": chunk,\n",
    "            \"original_context_length\": len(context_original),\n",
    "            \"without_chunk_context_length\": len(context_without),\n",
    "            # Rollout analysis\n",
    "            \"num_rollouts\": num_rollouts,\n",
    "            \"unique_responses\": unique_responses,\n",
    "            \"rollout_texts\": rollout_texts[:3],  # Store first 3 for inspection\n",
    "            # Similarity analysis\n",
    "            \"avg_cosine_similarity\": float(avg_cos_sim),\n",
    "            \"num_similar_to_original\": int(similar_mask.sum()),\n",
    "            \"num_different_from_original\": num_different,\n",
    "            \"counterfactual_importance\": kl_divergence,\n",
    "            # All cosine similarities for detailed analysis\n",
    "            \"all_cosine_similarities\": cos_sims.tolist()\n",
    "        })\n",
    "       \n",
    "        print(f\"  Generated {unique_responses}/{num_rollouts} unique responses\")\n",
    "        print(f\"  Avg cosine sim: {avg_cos_sim:.3f}, Different: {num_different}/{num_rollouts}\")\n",
    "\n",
    "# Print results\n",
    "for r in results:\n",
    "    print(f\"\\nChunk {r['chunk_index']}: {r['chunk_text'][:100]}...\")\n",
    "    print(f\"  Unique responses: {r['unique_responses']}/{r['num_rollouts']}\")\n",
    "    print(f\"  Counterfactual importance: {r['counterfactual_importance']}\")\n",
    "    print(f\"  Sample rollouts: {r['rollout_texts'][:2]}\")\n",
    "print(f\"\\nProcessed {len(results)} chunks with diverse generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dc415",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294bc83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"counterfactual_importance_results_control.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Results saved to counterfactual_importance_results_control.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
