{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def process_problem_data(base_path):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Iterates through all problem directories, extracts problem statements\n",
    "\n",
    "    and sentences from `chunks_labeled.json`, and returns a list of dictionaries.\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "        base_path (str): The path to the directory containing all the problems\n",
    "\n",
    "                         (e.g., 'math-rollouts/.../correct_base_solution').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        list: A list of dictionaries, where each dictionary contains the problem\n",
    "\n",
    "              and all sentences for a given problem directory.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_problem_data = []\n",
    "\n",
    "\n",
    "    # Check if the base path exists\n",
    "\n",
    "    if not os.path.isdir(base_path):\n",
    "\n",
    "        print(f\"Error: The directory '{base_path}' was not found.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "    print(f\"Found problem directory: {base_path}\")\n",
    "\n",
    "\n",
    "    # List all entries in the base directory\n",
    "\n",
    "    problem_dirs = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n",
    "\n",
    "    print(f\"Found problem directory: {problem_dirs}\")\n",
    "\n",
    "\n",
    "    if not problem_dirs:\n",
    "\n",
    "        print(f\"No problem directories found in '{base_path}'.\")\n",
    "\n",
    "        return all_problem_data\n",
    "\n",
    "\n",
    "    # Iterate through each problem directory (e.g., problem_330, problem_1591)\n",
    "\n",
    "    for problem_name in problem_dirs:\n",
    "\n",
    "        problem_path = os.path.join(base_path, problem_name)\n",
    "\n",
    "       \n",
    "\n",
    "        # Define the file paths for the problem and chunks\n",
    "\n",
    "        problem_file = os.path.join(problem_path, \"problem.json\")\n",
    "\n",
    "        chunks_file = os.path.join(problem_path, \"chunks_labeled.json\")\n",
    "\n",
    "       \n",
    "\n",
    "        problem_text = \"\"\n",
    "\n",
    "        allsentences = []\n",
    "\n",
    "       \n",
    "\n",
    "        # Load the problem statement\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(problem_file, 'r') as f:\n",
    "\n",
    "                problem_data = json.load(f)\n",
    "\n",
    "                problem_text = problem_data.get(\"problem\", \"\")\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load problem.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Load all sentences from chunks_labeled.json\n",
    "\n",
    "        try:\n",
    "\n",
    "            with open(chunks_file, 'r') as f:\n",
    "\n",
    "                chunks_data = json.load(f)\n",
    "\n",
    "                allsentences = [chunk[\"chunk\"] for chunk in chunks_data]\n",
    "\n",
    "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "\n",
    "            print(f\"Skipping {problem_name}: Could not load chunks_labeled.json. Error: {e}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Create a dictionary to store the extracted data\n",
    "\n",
    "        problem_info = {\n",
    "\n",
    "            \"problem_id\": problem_name,\n",
    "\n",
    "            \"problem_statement\": problem_text,\n",
    "\n",
    "            \"sentences\": allsentences\n",
    "\n",
    "        }\n",
    "\n",
    "        all_problem_data.append(problem_info)\n",
    "\n",
    "\n",
    "    return all_problem_data\n",
    "\n",
    "    print(\"No data was loaded.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/correct_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "correct_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "# Now, `all_data` is a list of dictionaries. You can iterate through it.\n",
    "\n",
    "print(f\"Successfully loaded data for {len(correct_all_data)} problems.\")\n",
    "\n",
    "# Define the base directory for all problems\n",
    "\n",
    "base_problem_dir = \"deepseek-r1-distill-llama-8b/temperature_0.6_top_p_0.95/incorrect_base_solution\"\n",
    "\n",
    "# Run the function to get all the data\n",
    "\n",
    "incorrect_all_data = process_problem_data(base_problem_dir)\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(incorrect_all_data)} problems.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = correct_all_data + incorrect_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f30d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" # Or any other suitable model\n",
    "\n",
    "mname = model_name\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Important: Add a pad token if the tokenizer doesn't have one, especially for decoder models.\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True,  torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6352bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: Sentence boundaries using tokenizer ---\n",
    "def get_raw_tokens(text, model_name=None):\n",
    "    return tokenizer(text)['input_ids']\n",
    "\n",
    "def get_sentence_token_boundaries(text, sentences, model_name=None):\n",
    "    import re\n",
    "    def normalize_spaces(s):\n",
    "        return re.sub(r\"[\\u00A0\\u1680\\u2000-\\u200B\\u202F\\u205F\\u3000\\uFEFF]\", \" \", s)\n",
    "    char_positions = []\n",
    "    search_start = 0\n",
    "    text_normalized = normalize_spaces(text)\n",
    "    for sentence in sentences:\n",
    "        sentence_normalized = normalize_spaces(sentence)\n",
    "        norm_pos = text_normalized.find(sentence_normalized, search_start)\n",
    "        if norm_pos == -1:\n",
    "            sentence_stripped = sentence_normalized.strip()\n",
    "            norm_pos = text_normalized.find(sentence_stripped, search_start)\n",
    "            if norm_pos == -1:\n",
    "                raise ValueError(f\"Sentence not found in text: {sentence}\")\n",
    "            norm_end = norm_pos + len(sentence_stripped)\n",
    "        else:\n",
    "            norm_end = norm_pos + len(sentence_normalized)\n",
    "        original_pos = 0\n",
    "        normalized_count = 0\n",
    "        actual_start = -1\n",
    "        actual_end = -1\n",
    "        for i, char in enumerate(text):\n",
    "            if normalized_count == norm_pos and actual_start == -1:\n",
    "                actual_start = i\n",
    "            if normalized_count == norm_end:\n",
    "                actual_end = i\n",
    "                break\n",
    "            if normalize_spaces(char) == \" \" or char == text_normalized[normalized_count]:\n",
    "                normalized_count += 1\n",
    "        if actual_end == -1 and normalized_count == norm_end:\n",
    "            actual_end = len(text)\n",
    "        char_positions.append((actual_start, actual_end))\n",
    "        search_start = norm_end\n",
    "    token_boundaries = []\n",
    "    for char_start, char_end in char_positions:\n",
    "        tokens_to_start = len(get_raw_tokens(text[:char_start], model_name)) if char_start > 0 else 0\n",
    "        tokens_to_end = len(get_raw_tokens(text[:char_end], model_name))\n",
    "        token_boundaries.append((tokens_to_start, tokens_to_end))\n",
    "    return token_boundaries\n",
    "\n",
    "# --- Helper: Average attention over sentence boundaries ---\n",
    "def _compute_averaged_matrix(matrix, sentence_boundaries):\n",
    "    n = len(sentence_boundaries)\n",
    "    result = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        row_start, row_end = sentence_boundaries[i]\n",
    "        row_start = min(row_start, matrix.shape[0] - 1)\n",
    "        row_end = min(row_end, matrix.shape[0] - 1)\n",
    "        if row_start >= row_end:\n",
    "            continue\n",
    "        for j in range(n):\n",
    "            col_start, col_end = sentence_boundaries[j]\n",
    "            col_start = min(col_start, matrix.shape[1] - 1)\n",
    "            col_end = min(col_end, matrix.shape[1] - 1)\n",
    "            if col_start >= col_end:\n",
    "                continue\n",
    "            region = matrix[row_start:row_end, col_start:col_end]\n",
    "            if region.size > 0:\n",
    "                result[i, j] = np.mean(region)\n",
    "    return result\n",
    "\n",
    "# --- Helper: Get vertical scores (receiver heads) ---\n",
    "def get_vertical_scores(avg_mat, proximity_ignore=1, control_depth=False, score_type=\"mean\"):\n",
    "    n = avg_mat.shape[0]\n",
    "    trius = np.triu_indices_from(avg_mat, k=1)\n",
    "    avg_mat = avg_mat.copy()\n",
    "    avg_mat[trius] = np.nan\n",
    "    trils = np.triu_indices_from(avg_mat, k=-proximity_ignore + 1)\n",
    "    avg_mat[trils] = np.nan\n",
    "    if control_depth:\n",
    "        per_row = np.sum(~np.isnan(avg_mat), axis=1)\n",
    "        avg_mat = stats.rankdata(avg_mat, axis=1, nan_policy=\"omit\") / per_row[:, None]\n",
    "    n = avg_mat.shape[-1]\n",
    "    vert_scores = []\n",
    "    for i in range(n):\n",
    "        vert_lines = avg_mat[i + proximity_ignore :, i]\n",
    "        if score_type == \"mean\":\n",
    "            vert_score = np.nanmean(vert_lines)\n",
    "        elif score_type == \"median\":\n",
    "            vert_score = np.nanmedian(vert_lines)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown score_type: {score_type}\")\n",
    "        vert_scores.append(vert_score)\n",
    "    return np.array(vert_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_ar_kurtosis(all_layer_head_vert_scores):\n",
    "    \"\"\"\n",
    "    Compute kurtosis across the last axis (sentences) for each (layer, head).\n",
    "    Input: all_layer_head_vert_scores: shape (num_layers, num_heads, num_sentences)\n",
    "    Output: layer_head_kurtosis: shape (num_layers, num_heads)\n",
    "    \"\"\"\n",
    "    return stats.kurtosis(\n",
    "        all_layer_head_vert_scores, axis=2, fisher=True, bias=True, nan_policy=\"omit\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_problem_optimized_with_running_attention_sum(problem_data, model, tokenizer, model_name):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Optimized version that returns vertical scores and attention weights for running sum.\n",
    "\n",
    "   \n",
    "\n",
    "    Returns:\n",
    "\n",
    "        dict: {\n",
    "\n",
    "            'vertical_scores': np.array shape (num_layers, num_heads, num_sentences),\n",
    "\n",
    "            'attention_weights': np.array shape (num_layers, num_heads, seq_len, seq_len),\n",
    "\n",
    "            'seq_len': int,\n",
    "\n",
    "            'problem_id': str\n",
    "\n",
    "        } or None if failed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Extract problem data\n",
    "\n",
    "        problem_statement = problem_data['problem_statement']\n",
    "\n",
    "        sentences = problem_data['sentences']\n",
    "\n",
    "       \n",
    "\n",
    "        # Limit sentence count to avoid memory issues\n",
    "\n",
    "        max_sentences = 100\n",
    "\n",
    "        if len(sentences) > max_sentences:\n",
    "\n",
    "            sentences = sentences[:max_sentences]\n",
    "\n",
    "       \n",
    "\n",
    "        # Reconstruct input_text\n",
    "\n",
    "        input_text = problem_statement + \"\\n\" + \"\\n\".join(sentences)\n",
    "\n",
    "       \n",
    "\n",
    "        # Tokenize with smaller max_length to reduce memory usage\n",
    "\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "\n",
    "       \n",
    "\n",
    "        # Move to GPU if available\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "       \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "        attention_weights = outputs.attentions\n",
    "\n",
    "       \n",
    "\n",
    "        # Get sentence boundaries\n",
    "\n",
    "        sentence_boundaries = get_sentence_token_boundaries(input_text, sentences, model_name)\n",
    "\n",
    "       \n",
    "\n",
    "        if len(sentence_boundaries) == 0:\n",
    "\n",
    "            return None\n",
    "\n",
    "       \n",
    "\n",
    "        # Get dimensions\n",
    "\n",
    "        num_layers = len(attention_weights)\n",
    "\n",
    "        num_heads = attention_weights[0].shape[1]\n",
    "\n",
    "        seq_len = attention_weights[0].shape[-1]\n",
    "\n",
    "        num_sentences = len(sentence_boundaries)\n",
    "\n",
    "       \n",
    "\n",
    "        # Initialize arrays\n",
    "\n",
    "        all_layer_head_vert_scores = np.zeros((num_layers, num_heads, num_sentences), dtype=np.float32)\n",
    "\n",
    "        attention_matrices = np.zeros((num_layers, num_heads, seq_len, seq_len), dtype=np.float32)\n",
    "\n",
    "       \n",
    "\n",
    "        for layer in range(num_layers):\n",
    "\n",
    "            for head in range(num_heads):\n",
    "\n",
    "                # FIX: Convert BFloat16 to Float32 BEFORE converting to NumPy\n",
    "\n",
    "                attn_tensor = attention_weights[layer][0, head].float()  # BFloat16 -> Float32\n",
    "\n",
    "                attn_mat = attn_tensor.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "               \n",
    "\n",
    "                # Store raw attention matrix\n",
    "\n",
    "                attention_matrices[layer, head] = attn_mat\n",
    "\n",
    "               \n",
    "\n",
    "                # Compute vertical scores\n",
    "\n",
    "                avg_mat = _compute_averaged_matrix(attn_mat, sentence_boundaries)\n",
    "\n",
    "                vert_scores = get_vertical_scores(avg_mat, proximity_ignore=1, control_depth=False, score_type=\"mean\")\n",
    "\n",
    "               \n",
    "\n",
    "                # Handle case where vert_scores might be shorter than expected\n",
    "\n",
    "                actual_len = min(len(vert_scores), num_sentences)\n",
    "\n",
    "                all_layer_head_vert_scores[layer, head, :actual_len] = vert_scores[:actual_len]\n",
    "\n",
    "       \n",
    "\n",
    "        # Clear GPU memory\n",
    "\n",
    "        del outputs, attention_weights\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "       \n",
    "\n",
    "        return {\n",
    "\n",
    "            'vertical_scores': all_layer_head_vert_scores,\n",
    "\n",
    "            'attention_weights': attention_matrices,\n",
    "\n",
    "            'seq_len': seq_len,\n",
    "\n",
    "            'problem_id': problem_data['problem_id']\n",
    "\n",
    "        }\n",
    "\n",
    "       \n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error processing problem {problem_data['problem_id']}: {e}\")\n",
    "\n",
    "        # Clear GPU memory on error\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "def quick_test_attention_processing(all_data, model, tokenizer, model_name, max_problems=5):\n",
    "    \"\"\"\n",
    "    Quick test version - processes only a few problems for rapid testing.\n",
    "    \"\"\"\n",
    "    print(f\"QUICK TEST: Processing only {max_problems} problems...\")\n",
    "    \n",
    "    # Limit to first few problems\n",
    "    test_data = all_data[:max_problems]\n",
    "    \n",
    "    successful_problems = 0\n",
    "    all_problem_vert_scores = []\n",
    "    \n",
    "    for i, problem in enumerate(test_data):\n",
    "        print(f\"Quick test {i+1}/{len(test_data)}: {problem['problem_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract and limit data aggressively\n",
    "            problem_statement = problem['problem_statement']\n",
    "            sentences = problem['sentences'][:10]  # Only first 10 sentences\n",
    "            \n",
    "            # Create very short input\n",
    "            input_text = problem_statement + \"\\n\" + \"\\n\".join(sentences)\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=256)  # Very short\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_attentions=True)\n",
    "            attention_weights = outputs.attentions\n",
    "            \n",
    "            # Quick sentence boundaries\n",
    "            sentence_boundaries = get_sentence_token_boundaries(input_text, sentences, model_name)\n",
    "            \n",
    "            if len(sentence_boundaries) == 0:\n",
    "                print(f\"  SKIP: No boundaries\")\n",
    "                continue\n",
    "            \n",
    "            # Quick vertical scores computation\n",
    "            num_layers = len(attention_weights)\n",
    "            num_heads = attention_weights[0].shape[1]\n",
    "            num_sentences = len(sentence_boundaries)\n",
    "            \n",
    "            all_layer_head_vert_scores = np.zeros((num_layers, num_heads, num_sentences), dtype=np.float32)\n",
    "            \n",
    "            for layer in range(num_layers):\n",
    "                for head in range(num_heads):\n",
    "                    attn_tensor = attention_weights[layer][0, head].float()\n",
    "                    attn_mat = attn_tensor.detach().cpu().numpy().astype(np.float32)\n",
    "                    \n",
    "                    avg_mat = _compute_averaged_matrix(attn_mat, sentence_boundaries)\n",
    "                    vert_scores = get_vertical_scores(avg_mat, proximity_ignore=1, control_depth=False, score_type=\"mean\")\n",
    "                    \n",
    "                    actual_len = min(len(vert_scores), num_sentences)\n",
    "                    all_layer_head_vert_scores[layer, head, :actual_len] = vert_scores[:actual_len]\n",
    "            \n",
    "            all_problem_vert_scores.append(all_layer_head_vert_scores)\n",
    "            successful_problems += 1\n",
    "            print(f\"  SUCCESS! Shape: {all_layer_head_vert_scores.shape}\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del outputs, attention_weights\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nQuick test complete! {successful_problems}/{len(test_data)} successful\")\n",
    "    \n",
    "    if successful_problems > 0:\n",
    "        # Quick kurtosis computation\n",
    "        min_sentences = min(scores.shape[2] for scores in all_problem_vert_scores)\n",
    "        truncated_scores = [scores[:, :, :min_sentences] for scores in all_problem_vert_scores]\n",
    "        stacked_scores = np.stack(truncated_scores, axis=0)\n",
    "        averaged_scores = np.mean(stacked_scores, axis=0)\n",
    "        \n",
    "        layer_head_kurtosis = get_3d_ar_kurtosis(averaged_scores)\n",
    "        \n",
    "        print(f\"Quick kurtosis shape: {layer_head_kurtosis.shape}\")\n",
    "        print(f\"Quick kurtosis range: {np.nanmin(layer_head_kurtosis):.3f} to {np.nanmax(layer_head_kurtosis):.3f}\")\n",
    "        \n",
    "        return layer_head_kurtosis\n",
    "    \n",
    "    return None\n",
    "\n",
    "def process_all_problems_with_iterative_attention_sum(all_data, model, tokenizer, model_name, batch_size=1, max_problems=None):\n",
    "    \"\"\"\n",
    "    Process problems and maintain running sums for attention weights to save memory.\n",
    "    Uses iterative summation instead of storing all attention matrices.\n",
    "    \n",
    "    OPTIMIZATIONS:\n",
    "    - Smaller batch_size default (1 instead of 3)\n",
    "    - max_problems parameter to limit total processing\n",
    "    - More frequent memory cleanup\n",
    "    \"\"\"\n",
    "    # OPTIMIZATION: Limit total problems if specified\n",
    "    if max_problems is not None:\n",
    "        all_data = all_data[:max_problems]\n",
    "        print(f\"OPTIMIZATION: Limited to {max_problems} problems\")\n",
    "    \n",
    "    all_problem_vert_scores = []\n",
    "   \n",
    "    # Initialize variables for running attention sum\n",
    "    attention_sum = None\n",
    "    min_seq_len = float('inf')\n",
    "    successful_problems = 0\n",
    "   \n",
    "    print(f\"Processing {len(all_data)} problems in batches of {batch_size}...\")\n",
    "   \n",
    "    for batch_start in tqdm(range(0, len(all_data), batch_size), desc=\"Processing batches\"):\n",
    "        batch_end = min(batch_start + batch_size, len(all_data))\n",
    "        batch_problems = all_data[batch_start:batch_end]\n",
    "       \n",
    "        for i, problem in enumerate(batch_problems):\n",
    "            global_idx = batch_start + i\n",
    "            print(f\"Processing problem {global_idx+1}/{len(all_data)}: {problem['problem_id']}\")\n",
    "           \n",
    "            result = process_single_problem_optimized_with_running_attention_sum(problem, model, tokenizer, model_name)\n",
    "           \n",
    "            if result is not None:\n",
    "                # Add vertical scores to list as before\n",
    "                all_problem_vert_scores.append(result['vertical_scores'])\n",
    "               \n",
    "                # Handle attention weights with running sum\n",
    "                current_attention = result['attention_weights']\n",
    "                current_seq_len = result['seq_len']\n",
    "               \n",
    "                # Track minimum sequence length\n",
    "                min_seq_len = min(min_seq_len, current_seq_len)\n",
    "               \n",
    "                # Initialize attention_sum on first successful problem\n",
    "                if attention_sum is None:\n",
    "                    num_layers, num_heads = current_attention.shape[:2]\n",
    "                    attention_sum = np.zeros((num_layers, num_heads, current_seq_len, current_seq_len), dtype=np.float64)\n",
    "               \n",
    "                # If current problem has smaller seq_len, resize attention_sum\n",
    "                if current_seq_len < attention_sum.shape[-1]:\n",
    "                    print(f\"  Resizing attention_sum from {attention_sum.shape[-1]} to {current_seq_len}\")\n",
    "                    attention_sum = attention_sum[:, :, :current_seq_len, :current_seq_len]\n",
    "               \n",
    "                # Add current attention to running sum\n",
    "                current_seq_len_to_use = min(current_seq_len, attention_sum.shape[-1])\n",
    "                attention_sum += current_attention[:, :, :current_seq_len_to_use, :current_seq_len_to_use].astype(np.float64)\n",
    "                successful_problems += 1\n",
    "               \n",
    "                print(f\"  SUCCESS: Added to running sum. Total successful: {successful_problems}\")\n",
    "               \n",
    "                # Clean up current result to save memory immediately\n",
    "                del result, current_attention\n",
    "            else:\n",
    "                print(f\"  FAILED: Got None result\")\n",
    "       \n",
    "        # OPTIMIZATION: More aggressive memory cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "       \n",
    "        print(f\"Completed batch {batch_start//batch_size + 1}. Successfully processed {successful_problems} problems so far.\")\n",
    "   \n",
    "    # Compute final averaged attention weights by dividing the sum\n",
    "\n",
    "    if attention_sum is not None and successful_problems > 0:\n",
    "\n",
    "        averaged_attention_weights = (attention_sum / successful_problems).astype(np.float32)\n",
    "\n",
    "        print(f\"\\nFinal averaged attention weights shape: {averaged_attention_weights.shape}\")\n",
    "\n",
    "        print(f\"Based on {successful_problems} successful problems\")\n",
    "\n",
    "       \n",
    "\n",
    "        # Clean up the sum to free memory\n",
    "\n",
    "        del attention_sum\n",
    "\n",
    "    else:\n",
    "\n",
    "        averaged_attention_weights = None\n",
    "\n",
    "        print(\"\\nNo successful problems processed\")\n",
    "\n",
    "   \n",
    "\n",
    "    return {\n",
    "\n",
    "        'all_problem_vert_scores': all_problem_vert_scores,\n",
    "\n",
    "        'averaged_attention_weights': averaged_attention_weights,\n",
    "\n",
    "        'successful_problems': successful_problems,\n",
    "\n",
    "        'min_seq_len': min_seq_len if min_seq_len != float('inf') else 0\n",
    "\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578aa633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "\n",
    "print(\"Starting memory-efficient processing with iterative attention summation...\")\n",
    "\n",
    "results = process_all_problems_with_iterative_attention_sum(\n",
    "    all_data, model, tokenizer, model_name, batch_size=4\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "\n",
    "print(f\"Successfully processed {results['successful_problems']} problems\")\n",
    "\n",
    "print(f\"Minimum sequence length: {results['min_seq_len']}\")\n",
    "\n",
    "\n",
    "if results['averaged_attention_weights'] is not None:\n",
    "\n",
    "    print(f\"Averaged attention weights shape: {results['averaged_attention_weights'].shape}\")\n",
    "\n",
    "   \n",
    "\n",
    "    # Store results for further analysis\n",
    "\n",
    "    all_problem_vert_scores = results['all_problem_vert_scores']\n",
    "\n",
    "    averaged_attention_weights = results['averaged_attention_weights']\n",
    "\n",
    "   \n",
    "\n",
    "    print(\"Memory-efficient processing with iterative summation completed successfully!\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"No attention weights were computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef10136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kurt_matrix(vertical_scores):\n",
    "  resp_layer_head_kurts = []\n",
    "  for i in range(len(vertical_scores)):\n",
    "    layer_head_verts  = vertical_scores[i]\n",
    "    kurtosis = get_3d_ar_kurtosis(layer_head_verts)\n",
    "    resp_layer_head_kurts.append(kurtosis)\n",
    "  \n",
    "  resp_layer_head_kurts = np.array(resp_layer_head_kurts)\n",
    "  resp_layer_head_kurts[:, 0, :] = np.nan  # ignore layer 0 (no interesting attention)\n",
    "  return resp_layer_head_kurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df2492",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results[\"all_problem_vert_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_head_kurtosis = get_kurt_matrix(results['all_problem_vert_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_head_kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_head_kurtosis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_kurtosis = np.nanmean(layer_head_kurtosis, axis=0)  # shape (32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1af95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attempt to do mean on vertical scores then kurt but, turns out it expected 3d\n",
    "# import copy\n",
    "# vert_scores_copy = copy.deepcopy(results['all_problem_vert_scores'])\n",
    "\n",
    "# # Find the minimum number of sentences\n",
    "# min_sentences = min(arr.shape[2] for arr in vert_scores_copy)\n",
    "\n",
    "# # Truncate all arrays to this minimum along the sentence axis\n",
    "# truncated_vert_scores = [arr[:, :, :min_sentences] for arr in vert_scores_copy]\n",
    "\n",
    "# # Now stack safely\n",
    "# stacked_vert_scores = np.stack(truncated_vert_scores, axis=0)\n",
    "\n",
    "# # Mean vertical score for each (layer, head) across all problems and all sentences\n",
    "# mean_vert_scores = np.nanmean(stacked_vert_scores, axis=(0, 3))  # shape: (num_layers, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a82aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "flat_kurtosis = layer_head_kurtosis.flatten()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(flat_kurtosis, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Kurtosis Scores for All (Layer, Head) Pairs')\n",
    "plt.xlabel('Kurtosis')\n",
    "plt.ylabel('Count')\n",
    "# Plot percentiles\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    perc = np.percentile(flat_kurtosis, p)\n",
    "    plt.axvline(perc, color='red', linestyle='--', label=f'{p}th percentile: {perc:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_kurtosis = layer_head_kurtosis.flatten()\n",
    "flat_kurtosis = flat_kurtosis[~np.isnan(flat_kurtosis)]  # Remove NaN values\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(flat_kurtosis, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Kurtosis Scores for All (Layer, Head) Pairs')\n",
    "plt.xlabel('Kurtosis')\n",
    "plt.ylabel('Count')\n",
    "# Plot percentiles\n",
    "for p in [50, 75, 90, 95, 99]:\n",
    "    perc = np.percentile(flat_kurtosis, p)\n",
    "    plt.axvline(perc, color='red', linestyle='--', label=f'{p}th percentile: {perc:.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_kurtosis = np.nanmean(layer_head_kurtosis, axis=0)  # shape (32, 32)\n",
    "mean_kurtosis[0, :] = np.nan  # Mask out layer 0\n",
    "\n",
    "num_top = 20  # Number of heads to select\n",
    "\n",
    "# Flatten and get indices of top 20 mean kurtosis values (ignoring NaNs)\n",
    "flat_indices = np.argsort(mean_kurtosis.flatten())[::-1]\n",
    "flat_indices = flat_indices[~np.isnan(mean_kurtosis.flatten()[flat_indices])][:num_top]\n",
    "layer_indices, head_indices = np.unravel_index(flat_indices, mean_kurtosis.shape)\n",
    "top_20_heads = list(zip(layer_indices, head_indices))\n",
    "\n",
    "print(\"Top 20 heads by mean kurtosis (layer, head):\")\n",
    "for i, (layer, head) in enumerate(top_20_heads):\n",
    "    print(f\"{i+1:2d}: Layer {layer}, Head {head}, Mean Kurtosis: {mean_kurtosis[layer, head]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9ff639",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(mean_kurtosis, cmap='viridis', annot=False, fmt=\".2f\")\n",
    "plt.title('Mean Kurtosis of Vertical Attention Scores by Layer and Head', fontsize=16)\n",
    "plt.xlabel('Heads')\n",
    "plt.ylabel('Layers')\n",
    "plt.xticks(np.arange(0, 32, 2), np.arange(0, 32, 2))\n",
    "plt.yticks(np.arange(0, 32, 2), np.arange(0, 32, 2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
